{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6cf4b19-b5c1-4374-8bd3-342ce231d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f414587-708f-49c1-9b13-659b9a6fbdd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25dfe938930>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32  # how many independent sequences will we process in parallel?\n",
    "block_size = 8   # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5146245-1bbe-4ded-9b53-0302ea94661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the data\n",
    "def get_training_data(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# Function to create mappings from characters to integers and vice versa\n",
    "def create_mappings(chars):\n",
    "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "    itos = {i: ch for i, ch in enumerate(chars)}\n",
    "    return stoi, itos\n",
    "\n",
    "# Function to create encoders and decoders using the mappings\n",
    "def create_encoders(stoi, itos):\n",
    "    encode = lambda s: [stoi[ch] for ch in s]  # encoder: take a string, output a list of integers\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n",
    "    return encode, decode\n",
    "\n",
    "# Function to prepare the data for character-level modeling\n",
    "def prepare_data(text, split_value):\n",
    "    # Determine unique characters\n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab_size = len(chars)\n",
    "    \n",
    "    # Create mappings from characters to integers and vice versa\n",
    "    stoi, itos = create_mappings(chars)\n",
    "    \n",
    "    # Encode the text\n",
    "    encode = lambda s: [stoi[ch] for ch in s]\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "    # Train and test splits\n",
    "    n = int(split_value * len(data))  # first 90% will be train, rest val\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "    \n",
    "    return train_data, val_data, vocab_size, stoi, itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8038972-5037-420a-83e9-f8017f129b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def get_batch(data):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Estimate loss function\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_data, val_data):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split_name, data in [('train', train_data), ('val', val_data)]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(data)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split_name] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e90e11a2-298d-4604-b2aa-a9dad5d741c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram model for character-level modeling\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)  # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]  # (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663f79cb-80c8-432b-9c8e-928dfcf36ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, optimizer, max_iters, eval_interval, train_data, val_data):\n",
    "    for iter in range(max_iters):\n",
    "        # Every once in a while, evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0:\n",
    "            losses = estimate_loss(model, train_data, val_data)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # Sample a batch of data\n",
    "        xb, yb = get_batch(train_data)\n",
    "\n",
    "        # Forward pass and loss computation\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "        # Backward pass and parameter update\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16d1d7fe-d162-4da3-bc0a-39d50c6ddcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Script\n",
    "\n",
    "# Load the data\n",
    "data = get_training_data('input.txt')\n",
    "\n",
    "# Prepare the data (includes extracting unique characters and creating mappings)\n",
    "train_data, val_data, vocab_size, stoi, itos = prepare_data(data, 0.8)\n",
    "\n",
    "# Create encoders and decoders\n",
    "encode, decode = create_encoders(stoi, itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0797a03-6480-46a3-bbfc-999ab389bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and optimizer\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdf1f340-737d-4f8c-9f66-a9b444c9f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.4622, val loss 2.5005\n",
      "step 300: train loss 2.4648, val loss 2.5116\n",
      "step 600: train loss 2.4569, val loss 2.5044\n",
      "step 900: train loss 2.4582, val loss 2.5154\n",
      "step 1200: train loss 2.4551, val loss 2.5028\n",
      "step 1500: train loss 2.4490, val loss 2.5149\n",
      "step 1800: train loss 2.4595, val loss 2.5019\n",
      "step 2100: train loss 2.4532, val loss 2.5006\n",
      "step 2400: train loss 2.4431, val loss 2.4958\n",
      "step 2700: train loss 2.4493, val loss 2.5139\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, optimizer, max_iters, eval_interval, train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9c1024e-bed1-4ab2-9af1-480572768d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_indices = model.generate(context, max_new_tokens=500)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efff15fc-ef68-48b1-b817-ec3853e4de22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HXEThaithicengr thasouamensh ofe stowncerestupar wind and ton herendseerthyoithew che\n",
      "Sateeser ig, y s cou MENaud main\n",
      "NCcow f ayook d ffo oig stas fry EDWher hof sutyo woss, me?\n",
      "KIs quchel by amy aiseerd findel cestothit theand,\n",
      "Teid wor stoun LERYofid ongu as. o tir aioVOF lodun,\n",
      "VI sor:\n",
      "Va hirerghien awof:\n",
      "d che, rewerd pak pe gounsticond fathtor mst, w hest d acef y 'd'linderalencter\n",
      "ONEYo sashol he\n",
      "\n",
      "Thugheald tcamore d lfaphonod, trast a we adam cesin hrefor M:\n",
      "PULIse t theara RD:\n",
      "forvouisp\n"
     ]
    }
   ],
   "source": [
    "print(decode(generated_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf19d48d-c660-4b31-abb5-74565a3dd4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hat MOUSThin OMERY$LELONELAn!\n",
      "Fan d anougee ngugof ESBunhenin\n",
      "Themithea he t of m, brelme:\n",
      "CI'l,\n",
      "Stimetowouins:\n",
      "\n",
      "\n",
      "Foll MAs he mbld y mathiar:\n",
      "f?\n",
      "\n",
      "Courepr; vee by, n thireroupromoson uls, baind moy\n",
      "AUKE:\n",
      "ARI:\n",
      "Awher y\n",
      "\n",
      "Serlewousiknaleazis\n",
      "SCK: s I tu storea f? nd.\n",
      "\n",
      "IES cors od we my:\n",
      "\n",
      "Brun?\n",
      "EOMy, ln whangorsh seprer ss? ublld d ls\n",
      "DIChonrimatatsesthimpr at hell o wiserlle s w, I k ilon bejofo g: de de\n",
      "The h tht.\n",
      "theaut t the lllar penendesturyoro an\n",
      "CE:\n",
      "Cle lime thealdac, per th fotheally;\n",
      "BY:\n",
      "Les\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_indices = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "\n",
    "print(decode(generated_indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
